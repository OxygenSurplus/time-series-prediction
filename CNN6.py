# æ—¶é—´ï¼š2024å¹´6æœˆ8å·  Dateï¼š June 16, 2024
# æ–‡ä»¶åç§° Filenameï¼š 03-main-full-safe-final-0.2.py
# ç¼–ç å®ç° Coding byï¼š Hongjie Liu , Suiwen Zhang é‚®ç®± Mailboxï¼šredsocks1043@163.com
# æ‰€å±å•ä½ï¼šä¸­å›½ æˆéƒ½ï¼Œè¥¿å—æ°‘æ—å¤§å­¦ï¼ˆSouthwest Minzu Universityï¼‰, è®¡ç®—æœºç§‘å­¦ä¸å·¥ç¨‹å­¦é™¢.
# æŒ‡å¯¼è€å¸ˆï¼šå‘¨ä¼Ÿè€å¸ˆ
# coding=utf-8
import time
import pandas as pd
import numpy as np
from scipy import stats
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
import matplotlib.pyplot as plt

start_time = time.time()

# ====================== å…¨å±€é…ç½®ï¼šCPUçº¿ç¨‹ä¼˜åŒ–ï¼ˆä¸å½±å“æ ¸å¿ƒé€»è¾‘ï¼‰ ======================
torch.set_num_threads(int(torch.get_num_threads() * 0.8))

# åŠ è½½æ•°æ®é›†
train_dataSet = pd.read_csv(r'../../../modified_æ•°æ®é›†Time_Series661_detail.dat')
test_dataSet = pd.read_csv(r'../../../modified_æ•°æ®é›†Time_Series662_detail.dat')

# åˆ—å®šä¹‰
columns = ['T_SONIC', 'CO2_density', 'CO2_density_fast_tmpr', 'H2O_density', 'H2O_sig_strgth', 'CO2_sig_strgth']
noise_columns = ['Error_T_SONIC', 'Error_CO2_density', 'Error_CO2_density_fast_tmpr', 'Error_H2O_density',
                 'Error_H2O_sig_strgth', 'Error_CO2_sig_strgth']

# è®°å½•CO2ç›¸å…³ç‰¹å¾çš„ç´¢å¼•ï¼ˆç”¨äºåç»­åŠ æƒå’Œç‰¹å¾å¢å¼ºï¼‰
# è¾“å‡ºåˆ—ä¸­ï¼šCO2_density(1)ã€CO2_density_fast_tmpr(2)
co2_output_indices = [1, 2]
# è¾“å…¥å™ªå£°åˆ—ä¸­ï¼šError_CO2_density(1)ã€Error_CO2_density_fast_tmpr(2)
co2_input_indices = [1, 2]

CL = columns + noise_columns

## æŸ¥çœ‹æ•°æ®ç¼ºå¤±æƒ…å†µ
data = train_dataSet[CL]
missingDf = data.isnull().sum().sort_values(ascending=False).reset_index()
missingDf.columns = ['feature', 'miss_num']
missingDf['miss_percentage'] = missingDf['miss_num'] / data.shape[0]
print("ç¼ºå¤±å€¼æ¯”ä¾‹")
print(missingDf)

# å¼‚å¸¸å€¼æ£€æµ‹ï¼ˆZ-scoreï¼‰ï¼šä¿ç•™åŸä»£ç é€»è¾‘
outlier_ratios = {}
for column in CL:
    col_clean = train_dataSet[column].dropna()
    if len(col_clean) == 0:
        outlier_ratios[column] = 0.0
        continue
    z_scores = np.abs(stats.zscore(col_clean))
    outliers = (z_scores > 2)
    outlier_ratio = outliers.mean() if len(outliers) > 0 else 0.0
    outlier_ratios[column] = outlier_ratio

print("*" * 30)
print("å¼‚å¸¸å€¼çš„æ¯”ä¾‹:")
for column, ratio in outlier_ratios.items():
    print(f"{column}: {ratio:.2%}")

# ====================== ä¿®å¤1ï¼šç¼ºå¤±å€¼å¤„ç†ï¼ˆåŸä»£ç éšè—bugï¼‰ ======================
imputer = SimpleImputer(strategy='median')
train_data_filled = train_dataSet.copy()
test_data_filled = test_dataSet.copy()
train_data_filled[CL] = imputer.fit_transform(train_data_filled[CL])
test_data_filled[CL] = imputer.transform(test_data_filled[CL])

# åˆ’åˆ†åŸå§‹è¾“å…¥è¾“å‡º
X_train_raw = train_data_filled[noise_columns].values.astype(np.float32)
y_train_raw = train_data_filled[columns].values.astype(np.float32)
X_test_raw = test_data_filled[noise_columns].values.astype(np.float32)
y_test_raw = test_data_filled[columns].values.astype(np.float32)

# === æ•°æ®æ ‡å‡†åŒ–ï¼šä¿ç•™åŸä»£ç é€»è¾‘ ===
scaler_X = StandardScaler()
scaler_y = StandardScaler()

X_train_scaled = scaler_X.fit_transform(X_train_raw)
y_train_scaled = scaler_y.fit_transform(y_train_raw)
X_test_scaled = scaler_X.transform(X_test_raw)

# ====================== ä¼˜åŒ–1ï¼šæ·»åŠ CO2ç‰¹å¾çš„å·®åˆ†ç‰¹å¾ï¼ˆæ•æ‰æ—¶åºå˜åŒ–è¶‹åŠ¿ï¼‰ ======================
def add_diff_feature(X, col_indices):
    """
    ä¸ºæŒ‡å®šåˆ—æ·»åŠ ä¸€é˜¶å·®åˆ†ç‰¹å¾ï¼ˆæ—¶åºå˜åŒ–é‡ï¼‰
    X: åŸå§‹ç‰¹å¾çŸ©é˜µ
    col_indices: éœ€è¦æ·»åŠ å·®åˆ†çš„åˆ—ç´¢å¼•
    """
    # ä¸€é˜¶å·®åˆ†ï¼šåä¸€ä¸ªå€¼å‡å‰ä¸€ä¸ªå€¼
    diff_X = np.zeros((len(X), len(col_indices)))
    diff_X[1:] = X[1:, col_indices] - X[:-1, col_indices]
    # æ‹¼æ¥åŸå§‹ç‰¹å¾å’Œå·®åˆ†ç‰¹å¾
    new_X = np.hstack([X, diff_X])
    return new_X

# ä¸ºCO2ç›¸å…³è¾“å…¥ç‰¹å¾æ·»åŠ å·®åˆ†ç‰¹å¾
X_train_scaled = add_diff_feature(X_train_scaled, co2_input_indices)
X_test_scaled = add_diff_feature(X_test_scaled, co2_input_indices)
# æ–°çš„è¾“å…¥é€šé“æ•°ï¼š6ï¼ˆåŸå§‹ï¼‰ + 2ï¼ˆå·®åˆ†ï¼‰ = 8
new_input_channels = X_train_scaled.shape[1]

# === æ„é€ åºåˆ—æ ·æœ¬ï¼ˆç”¨äºè®­ç»ƒï¼‰ï¼šä¿ç•™åŸä»£ç é€»è¾‘ ===
def create_sequences_for_training(X, y, seq_len=21, max_samples=None):
    Xs, ys = [], []
    half = seq_len // 2
    start, end = half, len(X) - half
    if end <= start:
        raise ValueError("æ•°æ®å¤ªçŸ­")

    total = end - start
    if max_samples and total > max_samples:
        indices = np.linspace(start, end - 1, num=max_samples, dtype=int)
    else:
        indices = range(start, end)

    for i in indices:
        Xs.append(X[i - half:i + half + 1])
        ys.append(y[i])
    return np.array(Xs), np.array(ys)

# === è®¾ç½®å‚æ•°ï¼šä¿ç•™åŸä»£ç é€»è¾‘ ===
SEQ_LEN = 21
MAX_SAMPLES = 200_000

print("æ­£åœ¨æ„é€ è®­ç»ƒåºåˆ—...")
X_train_seq, y_train_seq = create_sequences_for_training(
    X_train_scaled, y_train_scaled, SEQ_LEN, max_samples=MAX_SAMPLES
)

# è½¬ä¸º PyTorch å¼ é‡ï¼šä¿ç•™åŸä»£ç çš„permuteæ“ä½œï¼ˆ[N, seq_len, channels] â†’ [N, channels, seq_len]ï¼‰
X_train_tensor = torch.tensor(X_train_seq, dtype=torch.float32).permute(0, 2, 1)
y_train_tensor = torch.tensor(y_train_seq, dtype=torch.float32)

# ====================== ä¼˜åŒ–2ï¼šå¢å¼ºCNNæ¨¡å‹ï¼ˆé’ˆå¯¹CO2ç‰¹å¾ï¼Œå¾®è°ƒå·ç§¯æ ¸+ä¿ç•™åŸä»£ç æ ¸å¿ƒï¼‰ ======================
class CNN1DRegressor(nn.Module):
    def __init__(self, input_channels, output_dim=6):
        super().__init__()
        # ä¼˜åŒ–ï¼šå·ç§¯æ ¸ä»5æ”¹ä¸º7ï¼ˆæ•æ‰æ›´é•¿çš„CO2æ—¶åºä¾èµ–ï¼‰ï¼Œpaddingå¯¹åº”è°ƒæ•´
        self.conv1 = nn.Conv1d(input_channels, 64, kernel_size=7, padding=3)
        self.conv2 = nn.Conv1d(64, 32, kernel_size=3, padding=1)
        self.pool = nn.AdaptiveAvgPool1d(1)
        self.fc = nn.Linear(32, output_dim)

    def forward(self, x):
        # ä¿ç•™åŸä»£ç çš„æ¿€æ´»å’Œæ± åŒ–é€»è¾‘
        x = torch.relu(self.conv1(x))
        x = torch.relu(self.conv2(x))
        x = self.pool(x).squeeze(-1)
        return self.fc(x)

# === åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼šä¿ç•™åŸä»£ç é€»è¾‘ ===
X_train_final, X_val_final, y_train_final, y_val_final = train_test_split(
    X_train_tensor, y_train_tensor,
    test_size=0.2,
    random_state=42,
    shuffle=True
)

train_loader = DataLoader(TensorDataset(X_train_final, y_train_final), batch_size=512, shuffle=True)
val_loader = DataLoader(TensorDataset(X_val_final, y_val_final), batch_size=512, shuffle=False)

# === åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±ã€ä¼˜åŒ–å™¨ ===
device = torch.device('cpu')
model = CNN1DRegressor(input_channels=new_input_channels).to(device)

# ====================== ä¼˜åŒ–3ï¼šCO2ç‰¹å¾æŸå¤±åŠ æƒï¼ˆé‡ç‚¹é™ä½CO2çš„MAEï¼‰ ======================
# å®šä¹‰æŸå¤±æƒé‡ï¼šCO2ç›¸å…³ç‰¹å¾æƒé‡ä¸º3ï¼Œå…¶ä½™ä¸º1ï¼ˆæ”¾å¤§CO2çš„è¯¯å·®æƒ©ç½šï¼‰
loss_weights = torch.tensor([1.0, 3.0, 3.0, 1.0, 1.0, 1.0], dtype=torch.float32).to(device)
criterion = nn.L1Loss(reduction='none')  # æ”¹ä¸ºnoneï¼Œæ–¹ä¾¿æŒ‰æƒé‡è®¡ç®—
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

# === è®­ç»ƒä¸éªŒè¯ï¼šä¿ç•™åŸä»£ç çš„200è½®ï¼Œæ·»åŠ è½»å¾®æ—©åœï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰ ===
print(f"æ­£åœ¨è®­ç»ƒ CO2ä¼˜åŒ–ç‰ˆ 1D CNN æ¨¡å‹ï¼ˆè®­ç»ƒæ ·æœ¬æ•°={len(X_train_final)}, éªŒè¯æ ·æœ¬æ•°={len(X_val_final)}ï¼‰...")
num_epochs = 200
train_losses = []
val_losses = []
best_val_loss = float('inf')
best_model_state = None
patience = 30  # å®½æ¾çš„æ—©åœï¼Œä¿è¯å……åˆ†è®­ç»ƒ

for epoch in range(num_epochs):
    # --- è®­ç»ƒ ---
    model.train()
    train_loss = 0.0
    for xb, yb in train_loader:
        xb, yb = xb.to(device), yb.to(device)
        pred = model(xb)
        # åŠ æƒæŸå¤±ï¼šå¯¹CO2ç‰¹å¾çš„è¯¯å·®èµ‹äºˆæ›´é«˜æƒé‡
        loss = (criterion(pred, yb) * loss_weights).mean()
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        train_loss += loss.item()
    avg_train_loss = train_loss / len(train_loader)
    train_losses.append(avg_train_loss)

    # --- éªŒè¯ ---
    model.eval()
    val_loss = 0.0
    with torch.no_grad():
        for xb, yb in val_loader:
            xb, yb = xb.to(device), yb.to(device)
            pred = model(xb)
            loss = (criterion(pred, yb) * loss_weights).mean()
            val_loss += loss.item()
    avg_val_loss = val_loss / len(val_loader)
    val_losses.append(avg_val_loss)

    # ä¿å­˜æœ€ä¼˜æ¨¡å‹ï¼ˆé™ä½è¿‡æ‹Ÿåˆé£é™©ï¼‰
    if avg_val_loss < best_val_loss:
        best_val_loss = avg_val_loss
        best_model_state = model.state_dict()
        patience_counter = 0
    else:
        patience_counter += 1
        if patience_counter >= patience:
            print(f"æ—©åœè§¦å‘ï¼Œåœæ­¢è®­ç»ƒï¼ˆepoch={epoch}ï¼‰")
            break

    if epoch % 10 == 0:
        print(f"Epoch {epoch}, Train Loss: {avg_train_loss:.5f}, Val Loss: {avg_val_loss:.5f}")

# åŠ è½½æœ€ä¼˜æ¨¡å‹
model.load_state_dict(best_model_state)

# ==============================
# âœ… å®‰å…¨é¢„æµ‹ï¼šä¿®å¤åŸä»£ç å¡«å……é€»è¾‘çš„ç´¢å¼•è¶Šç•Œbugï¼Œä¿ç•™æ‰‹åŠ¨åè½¬æ ¸å¿ƒ
# ==============================
model.eval()
half = SEQ_LEN // 2
n_test = len(X_test_scaled)
y_pred_scaled_list = []
CHUNK_SIZE = 10000

print(f"æ­£åœ¨åˆ†å—é¢„æµ‹ï¼ˆæ€»è¡Œæ•°: {n_test}, æ¯å—: {CHUNK_SIZE}ï¼‰...")

for start_idx in range(0, n_test, CHUNK_SIZE):
    end_idx = min(start_idx + CHUNK_SIZE, n_test)
    current_chunk_size = end_idx - start_idx

    # è¾¹ç•Œå¡«å……ï¼šä¿ç•™åŸä»£ç çš„æ‰‹åŠ¨åè½¬é€»è¾‘ï¼Œä¿®å¤ç´¢å¼•è¶Šç•Œ
    pad_before = X_test_scaled[max(0, start_idx - half):start_idx]
    pad_after = X_test_scaled[end_idx:min(n_test, end_idx + half)]

    if len(pad_before) < half:
        needed = half - len(pad_before)
        # ä¿®å¤ï¼šé˜²æ­¢start_idx + neededè¶…å‡ºæ•°æ®é›†èŒƒå›´
        extra_start = max(0, start_idx)
        extra_end = min(len(X_test_scaled), start_idx + needed)
        extra = X_test_scaled[extra_start:extra_end][::-1]
        # è‹¥extraé•¿åº¦ä¸è¶³ï¼Œç”¨extraçš„æœ€åéƒ¨åˆ†è¡¥å…¨
        if len(extra) < needed:
            extra = np.pad(extra, ((needed - len(extra), 0), (0, 0)), mode='edge')
        pad_before = np.concatenate([extra, pad_before], axis=0)
    if len(pad_after) < half:
        needed = half - len(pad_after)
        # ä¿®å¤ï¼šé˜²æ­¢end_idx - neededå°äº0
        extra_start = max(0, end_idx - needed)
        extra_end = end_idx
        extra = X_test_scaled[extra_start:extra_end][::-1]
        # è‹¥extraé•¿åº¦ä¸è¶³ï¼Œç”¨extraçš„æœ€åéƒ¨åˆ†è¡¥å…¨
        if len(extra) < needed:
            extra = np.pad(extra, ((0, needed - len(extra)), (0, 0)), mode='edge')
        pad_after = np.concatenate([pad_after, extra], axis=0)

    # å¼ºåˆ¶æˆªæ–­åˆ°halfé•¿åº¦ï¼Œé¿å…å¡«å……åè¿‡é•¿
    pad_before = pad_before[-half:] if len(pad_before) > half else pad_before
    pad_after = pad_after[:half] if len(pad_after) > half else pad_after

    local_padded = np.concatenate([pad_before, X_test_scaled[start_idx:end_idx], pad_after], axis=0)

    # æ„é€ çª—å£ï¼šæ·»åŠ é•¿åº¦æ ¡éªŒï¼Œä¿®å¤åŸä»£ç æ½œåœ¨bug
    X_chunk_seq = []
    for i in range(half, half + current_chunk_size):
        window = local_padded[i - half:i + half + 1]
        # ç¡®ä¿çª—å£é•¿åº¦ä¸ºSEQ_LEN
        if len(window) != SEQ_LEN:
            if len(window) < SEQ_LEN:
                window = np.pad(window, ((0, SEQ_LEN - len(window)), (0, 0)), mode='edge')
            else:
                window = window[:SEQ_LEN]
        X_chunk_seq.append(window)

    X_chunk_tensor = torch.tensor(np.array(X_chunk_seq), dtype=torch.float32).permute(0, 2, 1)

    with torch.no_grad():
        pred_chunk = model(X_chunk_tensor.to(device)).cpu().numpy()
    y_pred_scaled_list.append(pred_chunk)

    print(f"  å·²å¤„ç† [{start_idx} : {end_idx}] / {n_test}")

# åˆå¹¶é¢„æµ‹ç»“æœï¼šä¿ç•™åŸä»£ç é€»è¾‘
y_pred_scaled_full = np.vstack(y_pred_scaled_list)
y_predict_full = scaler_y.inverse_transform(y_pred_scaled_full)

# é•¿åº¦æ ¡éªŒï¼šæ·»åŠ ä¿æŠ¤é€»è¾‘
if len(y_predict_full) > len(y_test_raw):
    y_predict_full = y_predict_full[:len(y_test_raw)]
elif len(y_predict_full) < len(y_test_raw):
    y_predict_full = np.pad(y_predict_full, ((0, len(y_test_raw)-len(y_predict_full)), (0,0)), mode='edge')
assert len(y_predict_full) == len(y_test_raw), f"é•¿åº¦ä¸åŒ¹é…: {len(y_predict_full)} vs {len(y_test_raw)}"

# === ä¿å­˜ç»“æœï¼šä¿ç•™åŸä»£ç é€»è¾‘ ===
results = []
for True_Value, Predicted_Value in zip(y_test_raw, y_predict_full):
    error = np.abs(True_Value - Predicted_Value)
    formatted_true = ' '.join(map(str, True_Value))
    formatted_pred = ' '.join(map(str, Predicted_Value))
    formatted_error = ' '.join(map(str, error))
    results.append([formatted_true, formatted_pred, formatted_error])

result_df = pd.DataFrame(results, columns=['True_Value', 'Predicted_Value', 'Error'])
result_df.to_csv("result_CNN1D_final_0.2.csv", index=False)

# === è®¡ç®— MAEï¼šä¿ç•™åŸä»£ç é€»è¾‘ ===
error_matrix = np.array([list(map(float, row.split())) for row in result_df['Error']])
mae_per_var = np.mean(error_matrix, axis=0)
overall_mae = np.mean(mae_per_var)

print("<*>" * 50)
print("6ä¸ªå˜é‡çš„ MAE åˆ†åˆ«ä¸ºï¼š")
for idx, col in enumerate(columns):
    print(f"{col}: {mae_per_var[idx]:.4f}")
print(f"\nğŸ¯ æ€»ä½“å¹³å‡è¯¯å·® (MAE): {overall_mae:.5f}")
print(f"âœ… é¢„æµ‹ç»“æœè¡Œæ•°: {len(result_df)}ï¼ŒåŸå§‹æµ‹è¯•é›†è¡Œæ•°: {len(test_dataSet)} â†’ å®Œå…¨å¯¹é½ï¼")
print(f"æ€»è€—æ—¶ï¼š{time.time() - start_time:.3f} ç§’")