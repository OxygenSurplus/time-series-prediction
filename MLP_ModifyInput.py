# æ—¶é—´ï¼š2024å¹´6æœˆ8å·  Dateï¼š June 16, 2024
# æ–‡ä»¶åç§° Filenameï¼š 03-main.py
# ç¼–ç å®ç° Coding byï¼š Hongjie Liu , Suiwen Zhang é‚®ç®± Mailboxï¼šredsocks1043@163.com
# æ‰€å±å•ä½ï¼šä¸­å›½ æˆéƒ½ï¼Œè¥¿å—æ°‘æ—å¤§å­¦ï¼ˆSouthwest Minzu Universityï¼‰, è®¡ç®—æœºç§‘å­¦ä¸å·¥ç¨‹å­¦é™¢.
# æŒ‡å¯¼è€å¸ˆï¼šå‘¨ä¼Ÿè€å¸ˆ
# coding=utf-8
import time
import pandas as pd
import numpy as np
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error
from scipy import stats

start_time = time.time()

# åŠ è½½æ•°æ®é›†
train_dataSet = pd.read_csv(r'../../modified_æ•°æ®é›†Time_Series661_detail.dat')
test_dataSet = pd.read_csv(r'../../modified_æ•°æ®é›†Time_Series662_detail.dat')

columns = ['T_SONIC', 'CO2_density', 'CO2_density_fast_tmpr', 'H2O_density', 'H2O_sig_strgth', 'CO2_sig_strgth']
noise_columns = ['Error_T_SONIC', 'Error_CO2_density', 'Error_CO2_density_fast_tmpr',
                 'Error_H2O_density', 'Error_H2O_sig_strgth', 'Error_CO2_sig_strgth']

CL = columns + noise_columns

## æŸ¥çœ‹æ•°æ®ç¼ºå¤±æƒ…å†µ
data = train_dataSet[CL]
missingDf = data.isnull().sum().sort_values(ascending=False).reset_index()
missingDf.columns = ['feature', 'miss_num']
missingDf['miss_percentage'] = missingDf['miss_num'] / data.shape[0]
print("ç¼ºå¤±å€¼æ¯”ä¾‹")
print(missingDf)

# åˆå§‹åŒ–ä¸€ä¸ªå­—å…¸æ¥å­˜å‚¨æ¯ä¸€åˆ—çš„å¼‚å¸¸å€¼æ¯”ä¾‹
outlier_ratios = {}
for column in CL:
    z_scores = np.abs(stats.zscore(train_dataSet[column]))
    outliers = (z_scores > 2)
    outlier_ratio = outliers.mean()
    outlier_ratios[column] = outlier_ratio

print("*" * 30)
print("å¼‚å¸¸å€¼çš„æ¯”ä¾‹:")
for column, ratio in outlier_ratios.items():
    print(f"{column}: {ratio:.2%}")

# # === å…³é”®ä¿®æ”¹ï¼šæ„é€ å¸¦å™ªè§‚æµ‹ Noisy_* = True + Error ===
# noisy_cols = []
# for col in columns:
#     err_col = f'Error_{col}'
#     noisy_col = f'Noisy_{col}'
#     train_dataSet[noisy_col] = train_dataSet[col] + train_dataSet[err_col]
#     test_dataSet[noisy_col] = test_dataSet[col] + test_dataSet[err_col]
#     noisy_cols.append(noisy_col)

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ˆä½¿ç”¨å¸¦å™ªè§‚æµ‹ä½œä¸ºè¾“å…¥ï¼‰
# X_train = train_dataSet[noisy_cols].values.astype(np.float32)
# y_train = train_dataSet[columns].values.astype(np.float32)
#
# X_test = test_dataSet[noisy_cols].values.astype(np.float32)
# y_test = test_dataSet[columns].values.astype(np.float32)

X_train = train_dataSet[noise_columns].values.astype(np.float32)
y_train = train_dataSet[columns].values.astype(np.float32)

X_test = test_dataSet[noise_columns].values.astype(np.float32)
y_test = test_dataSet[columns].values.astype(np.float32)
# ç‰¹å¾æ ‡å‡†åŒ–
scaler_X = StandardScaler()
X_train_scaled = scaler_X.fit_transform(X_train)
X_test_scaled = scaler_X.transform(X_test)

# === å®šä¹‰è½»é‡ MLP æ¨¡å‹ï¼ˆå¿«é€Ÿ + é«˜ç²¾åº¦ï¼‰===
print("å¼€å§‹è®­ç»ƒ MLP æ¨¡å‹...")
mlp = MLPRegressor(
    hidden_layer_sizes=(128, 64),
    activation='relu',
    solver='adam',
    alpha=1e-4,
    batch_size=256,
    learning_rate_init=1e-3,
    max_iter=300,
    early_stopping=True,
    validation_fraction=0.1,
    n_iter_no_change=10,
    random_state=42,
    verbose=False
)

# è®­ç»ƒ
mlp.fit(X_train_scaled, y_train)
print(f"âœ… MLP è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {time.time() - start_time:.2f} ç§’")

# é¢„æµ‹
y_predict = mlp.predict(X_test_scaled)

# ä¿å­˜ç»“æœï¼ˆä¿æŒä½ åŸæ¥çš„æ ¼å¼ï¼‰
results = []
for true_val, pred_val in zip(y_test, y_predict):
    error = np.abs(true_val - pred_val)
    formatted_true = ' '.join(map(str, true_val))
    formatted_pred = ' '.join(map(str, pred_val))
    formatted_error = ' '.join(map(str, error))
    results.append([formatted_true, formatted_pred, formatted_error])

result_df = pd.DataFrame(results, columns=['True_Value', 'Predicted_Value', 'Error'])
result_df.to_csv("result_MLP1.csv", index=False)

print("<*>" * 50)

# ä» CSV è¯»å–å¹¶è®¡ç®—å¹³å‡ MAEï¼ˆå…¼å®¹ä½ åŸæœ‰é€»è¾‘ï¼‰
data = pd.read_csv("result_MLP1.csv")
column3 = data.iloc[:, 2]
numbers = column3.str.split(' ', expand=True).apply(pd.to_numeric)
means = numbers.mean()
overall_mae = means.mean()

print("6ä¸ªå˜é‡çš„ MAE åˆ†åˆ«ä¸ºï¼š\n", means)
print(f"\nğŸ¯ æ€»ä½“ MAE: {overall_mae:.5f}")

end_time = time.time()
print(f"æ€»è€—æ—¶ï¼š{end_time - start_time:.3f} ç§’")